{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab3448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Best Model Training with Hyperparameter Tuning\n",
    "Complete pipeline: Grid search → Best model training → Sample evaluation\n",
    "Fixed at 5 epochs for final training\n",
    "\n",
    "CS-GY 6923 Scaling Laws Project\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from music21 import converter\n",
    "\n",
    "\n",
    "# Import the GPT model from document 3 (same architecture)\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 256\n",
    "    block_size: int = 256\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = False\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                            .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.size()\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=idx.device)\n",
    "        x = self.transformer.drop(self.transformer.wte(idx) + self.transformer.wpe(pos))\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        logits = self.lm_head(self.transformer.ln_f(x))\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) if targets is not None else None\n",
    "        return logits, loss\n",
    "\n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            idx = torch.cat((idx, torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data_path: str, vocab_path: str, block_size: int):\n",
    "        self.block_size = block_size\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(vocab)}\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        self.tokens = [self.char_to_idx.get(ch, 0) for ch in text]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.tokens[idx:idx + self.block_size + 1]\n",
    "        return torch.tensor(chunk[:-1], dtype=torch.long), torch.tensor(chunk[1:], dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.idx_to_char.get(i, '<UNK>') for i in indices])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Unified training configuration\"\"\"\n",
    "    model_name: str = 'best'\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    block_size: int = 512\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 5  # Fixed\n",
    "    learning_rate: float = 6e-4\n",
    "    weight_decay: float = 0.1\n",
    "    warmup_steps: int = 2000\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    eval_interval: int = 500\n",
    "    save_interval: int = 1000\n",
    "\n",
    "    data_dir: str = \"/content/drive/MyDrive/processed_music_data\"\n",
    "    output_dir: str = \"/content/drive/MyDrive/models/best_model\"\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def get_lr(step, config):\n",
    "    \"\"\"Cosine LR with warmup\"\"\"\n",
    "    if step < config.warmup_steps:\n",
    "        return config.learning_rate * step / config.warmup_steps\n",
    "    total = len(train_loader) * config.epochs if 'train_loader' in globals() else 10000\n",
    "    progress = (step - config.warmup_steps) / (total - config.warmup_steps)\n",
    "    return config.learning_rate * 0.1 + 0.9 * config.learning_rate * (1 + math.cos(math.pi * progress)) / 2\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, max_iters=100):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if i >= max_iters:\n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, config, best_val_loss, step=0):\n",
    "    \"\"\"Save training checkpoint\"\"\"\n",
    "    checkpoint_dir = Path(config.output_dir) / \"checkpoints\"\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'config': asdict(config)\n",
    "    }\n",
    "\n",
    "    # Save latest checkpoint\n",
    "    latest_path = checkpoint_dir / \"checkpoint_latest.pt\"\n",
    "    torch.save(checkpoint, latest_path)\n",
    "\n",
    "    # Also save epoch-specific checkpoint\n",
    "    epoch_path = checkpoint_dir / f\"checkpoint_epoch_{epoch}.pt\"\n",
    "    torch.save(checkpoint, epoch_path)\n",
    "\n",
    "    print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "    return latest_path\n",
    "\n",
    "\n",
    "def load_checkpoint(config):\n",
    "    \"\"\"Load the latest checkpoint if exists\"\"\"\n",
    "    checkpoint_dir = Path(config.output_dir) / \"checkpoints\"\n",
    "    latest_path = checkpoint_dir / \"checkpoint_latest.pt\"\n",
    "\n",
    "    if latest_path.exists():\n",
    "        print(f\"Found checkpoint: {latest_path}\")\n",
    "        checkpoint = torch.load(latest_path, map_location='cpu', weights_only=False)\n",
    "        return checkpoint\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def train_model(config: TrainingConfig, is_tuning=False):\n",
    "    \"\"\"Core training function with auto-resume capability\"\"\"\n",
    "    device = torch.device(config.device)\n",
    "\n",
    "    # Load data\n",
    "    vocab_path = Path(config.data_dir) / \"vocab.json\"\n",
    "    with open(vocab_path) as f:\n",
    "        vocab = json.load(f)\n",
    "\n",
    "    train_ds = MusicDataset(Path(config.data_dir) / \"train.txt\", vocab_path, config.block_size)\n",
    "    val_ds = MusicDataset(Path(config.data_dir) / \"val.txt\", vocab_path, config.block_size)\n",
    "    test_ds = MusicDataset(Path(config.data_dir) / \"test.txt\", vocab_path, config.block_size)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=config.batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model_config = GPTConfig(\n",
    "        vocab_size=len(vocab),\n",
    "        block_size=config.block_size,\n",
    "        n_layer=config.n_layer,\n",
    "        n_head=config.n_head,\n",
    "        n_embd=config.n_embd,\n",
    "        dropout=config.dropout\n",
    "    )\n",
    "    model = GPT(model_config).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "    # Initialize training state\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    global_step = 0\n",
    "\n",
    "    # AUTO-RESUME: Try to load checkpoint\n",
    "    checkpoint = load_checkpoint(config)\n",
    "    if checkpoint is not None:\n",
    "        print(\"RESUMING FROM CHECKPOINT\")\n",
    "        print(\"=\"*80)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        global_step = checkpoint.get('step', 0)\n",
    "        print(f\"Resumed from epoch {start_epoch}\")\n",
    "        print(f\"   Best val loss so far: {best_val_loss:.4f}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting fresh training.\\n\")\n",
    "\n",
    "    # Training loop\n",
    "    try:\n",
    "        for epoch in range(start_epoch, config.epochs):\n",
    "            model.train()\n",
    "            epoch_losses = []\n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.epochs}\")\n",
    "\n",
    "            for batch_idx, (x, y) in enumerate(pbar):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                _, loss = model(x, y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "                optimizer.step()\n",
    "\n",
    "                lr = get_lr(global_step, config)\n",
    "                for pg in optimizer.param_groups:\n",
    "                    pg['lr'] = lr\n",
    "\n",
    "                epoch_losses.append(loss.item())\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{lr:.2e}'})\n",
    "\n",
    "                # Periodic evaluation\n",
    "                if global_step % config.eval_interval == 0 and global_step > 0:\n",
    "                    val_loss = evaluate(model, val_loader, device)\n",
    "                    print(f\"\\nStep {global_step}: val_loss = {val_loss:.4f}\")\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        print(f\"New best model!\")\n",
    "\n",
    "                    model.train()\n",
    "\n",
    "                # Save checkpoint every save_interval steps\n",
    "                if global_step % config.save_interval == 0 and global_step > 0:\n",
    "                    save_checkpoint(model, optimizer, epoch, config, best_val_loss, global_step)\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "            # End of epoch: ALWAYS save checkpoint\n",
    "            print(f\"\\nEnd of epoch {epoch+1} - Saving checkpoint...\")\n",
    "            save_checkpoint(model, optimizer, epoch + 1, config, best_val_loss, global_step)\n",
    "\n",
    "            # Epoch summary\n",
    "            train_loss = np.mean(epoch_losses)\n",
    "            val_loss = evaluate(model, val_loader, device)\n",
    "\n",
    "            print(f\"\\nEpoch {epoch+1}/{config.epochs} Summary:\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "            print(f\"  Best Val:   {best_val_loss:.4f}\\n\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user!\")\n",
    "        print(\"Saving emergency checkpoint...\")\n",
    "        save_checkpoint(model, optimizer, epoch, config, best_val_loss, global_step)\n",
    "        print(\"Emergency checkpoint saved. You can resume training later.\")\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during training: {e}\")\n",
    "        print(\"Saving emergency checkpoint...\")\n",
    "        save_checkpoint(model, optimizer, epoch, config, best_val_loss, global_step)\n",
    "        print(\"Emergency checkpoint saved.\")\n",
    "        raise\n",
    "\n",
    "    # Final eval\n",
    "    test_loss = evaluate(model, test_loader, device, max_iters=999999)\n",
    "    test_ppl = np.exp(test_loss)\n",
    "\n",
    "    # Generate samples if not tuning\n",
    "    samples = []\n",
    "    if not is_tuning:\n",
    "        samples = generate_samples(model, train_ds, config, device)\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'val_loss': best_val_loss,\n",
    "        'test_loss': test_loss,\n",
    "        'test_perplexity': test_ppl,\n",
    "        'samples': samples\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_samples(model, dataset, config, device):\n",
    "    \"\"\"Generate and evaluate samples\"\"\"\n",
    "    model.eval()\n",
    "    samples = []\n",
    "\n",
    "    for temp in [0.8, 0.9, 1.0]:\n",
    "        for i in range(5):\n",
    "            idx = torch.tensor([[0]], device=device)\n",
    "            generated = model.generate(idx, max_new_tokens=512, temperature=temp, top_k=50)\n",
    "            text = dataset.decode(generated[0].tolist())\n",
    "            samples.append({'temp': temp, 'text': text, 'valid': len(text) > 50})\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "def hyperparameter_tuning(base_config: TrainingConfig):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HYPERPARAMETER TUNING (2 epochs per config)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # REDUCED SEARCH SPACE - Only 4 configs due to limited compute\n",
    "    search_space = [\n",
    "        # (lr, dropout, weight_decay)\n",
    "        (3e-4, 0.1, 0.1),   # Conservative baseline\n",
    "        (6e-4, 0.1, 0.1),   # Standard GPT-style\n",
    "        (1e-3, 0.1, 0.0),   # Higher LR, no weight decay\n",
    "        (6e-4, 0.0, 0.1),   # No dropout\n",
    "    ]\n",
    "\n",
    "    combinations = search_space\n",
    "\n",
    "    print(f\"Testing {len(combinations)} carefully selected configurations\")\n",
    "    print(\"This will save ~75% compute vs full grid search\\n\")\n",
    "\n",
    "    results = []\n",
    "    for idx, (lr, dropout, wd) in enumerate(combinations):\n",
    "        print(f\"\\nConfig {idx+1}/{len(combinations)}: LR={lr:.2e}, Dropout={dropout:.2f}, WD={wd:.2f}\")\n",
    "\n",
    "        config = TrainingConfig(\n",
    "            model_name=f'tune_{idx+1}',\n",
    "            n_layer=base_config.n_layer,\n",
    "            n_head=base_config.n_head,\n",
    "            n_embd=base_config.n_embd,\n",
    "            block_size=base_config.block_size,\n",
    "            dropout=dropout,\n",
    "            learning_rate=lr,\n",
    "            weight_decay=wd,\n",
    "            epochs=2,\n",
    "            batch_size=base_config.batch_size,\n",
    "            data_dir=base_config.data_dir,\n",
    "            output_dir=f\"{base_config.output_dir}/tuning/config_{idx+1}\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = train_model(config, is_tuning=True)\n",
    "            results.append({\n",
    "                'config_id': idx+1,\n",
    "                'lr': lr,\n",
    "                'dropout': dropout,\n",
    "                'weight_decay': wd,\n",
    "                'val_loss': result['val_loss'],\n",
    "                'test_ppl': result['test_perplexity']\n",
    "            })\n",
    "            print(f\"Val Loss: {result['val_loss']:.4f}, Test PPL: {result['test_perplexity']:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {e}\")\n",
    "            results.append({'config_id': idx+1, 'lr': lr, 'dropout': dropout, 'weight_decay': wd,\n",
    "                           'val_loss': float('inf'), 'test_ppl': float('inf')})\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Sort by val loss\n",
    "    results.sort(key=lambda x: x['val_loss'])\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TUNING RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    for rank, r in enumerate(results[:5], 1):\n",
    "        print(f\"{rank}. LR={r['lr']:.2e}, Dropout={r['dropout']:.2f}, WD={r['weight_decay']:.2f} \"\n",
    "              f\"→ Val={r['val_loss']:.4f}, PPL={r['test_ppl']:.2f}\")\n",
    "\n",
    "    # Save results\n",
    "    tuning_dir = Path(base_config.output_dir) / \"tuning\"\n",
    "    tuning_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with open(tuning_dir / \"results.json\", 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    best = results[0]\n",
    "    print(f\"\\nBEST: LR={best['lr']:.2e}, Dropout={best['dropout']:.2f}, WD={best['weight_decay']:.2f}\")\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Complete pipeline with tuning + final training + sample generation\"\"\"\n",
    "\n",
    "    base_config = TrainingConfig(\n",
    "        n_layer=12,\n",
    "        n_head=12,\n",
    "        n_embd=768,\n",
    "        block_size=512,\n",
    "        epochs=2,\n",
    "        data_dir=\"/content/drive/MyDrive/processed_music_data\",\n",
    "        output_dir=\"/content/drive/MyDrive/models/best_model\"\n",
    "    )\n",
    "\n",
    "    # Tuning\n",
    "    print(\"\\nHYPERPARAMETER TUNING\")\n",
    "    best_hp = hyperparameter_tuning(base_config)\n",
    "\n",
    "    # Train with best hyperparameters + sample generation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL TRAINING WITH BEST HYPERPARAMETERS + GENERATE SAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    final_config = TrainingConfig(\n",
    "        model_name='final_best',\n",
    "        n_layer=base_config.n_layer,\n",
    "        n_head=base_config.n_head,\n",
    "        n_embd=base_config.n_embd,\n",
    "        block_size=base_config.block_size,\n",
    "        dropout=best_hp['dropout'],\n",
    "        learning_rate=best_hp['lr'],\n",
    "        weight_decay=best_hp['weight_decay'],\n",
    "        epochs=5,\n",
    "        batch_size=base_config.batch_size,\n",
    "        data_dir=base_config.data_dir,\n",
    "        output_dir=f\"{base_config.output_dir}/final\"\n",
    "    )\n",
    "\n",
    "    result = train_model(final_config, is_tuning=False)\n",
    "\n",
    "    # Save samples\n",
    "    samples_dir = Path(final_config.output_dir) / \"samples\"\n",
    "    samples_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for i, s in enumerate(result['samples']):\n",
    "        with open(samples_dir / f\"sample_{i+1:03d}_temp{s['temp']}.txt\", 'w') as f:\n",
    "            f.write(s['text'])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Test Perplexity: {result['test_perplexity']:.2f}\")\n",
    "    print(f\"Valid Samples: {sum(1 for s in result['samples'] if s['valid'])}/{len(result['samples'])}\")\n",
    "    print(f\"Results: {final_config.output_dir}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
