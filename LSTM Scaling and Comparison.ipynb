{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcpas8Db7GKX"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LSTM Scaling Study - Complete Training Pipeline\n",
    "Standard LSTM architecture with modifications for music data\n",
    "\n",
    "CS-GY 6923 Scaling Laws Project\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class LSTMConfig:\n",
    "    \"\"\"LSTM model configuration\"\"\"\n",
    "    vocab_size: int = 256\n",
    "    embedding_dim: int = 128\n",
    "    hidden_dim: int = 256\n",
    "    num_layers: int = 2\n",
    "    dropout: float = 0.2\n",
    "    bidirectional: bool = False\n",
    "\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    \"\"\"Standard LSTM for language modeling\"\"\"\n",
    "\n",
    "    def __init__(self, config: LSTMConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=config.embedding_dim,\n",
    "            hidden_size=config.hidden_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout if config.num_layers > 1 else 0.0,\n",
    "            bidirectional=config.bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # Output layer\n",
    "        lstm_output_dim = config.hidden_dim * (2 if config.bidirectional else 1)\n",
    "        self.fc = nn.Linear(lstm_output_dim, config.vocab_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        # Embedding initialization\n",
    "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
    "\n",
    "        # LSTM initialization\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "                # Set forget gate bias to 1\n",
    "                n = param.size(0)\n",
    "                param.data[n//4:n//2].fill_(1)\n",
    "\n",
    "        # Linear layer initialization\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hidden: Optional[Tuple] = None) -> Tuple[torch.Tensor, Tuple, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Args:\n",
    "            x: input tensor [batch_size, seq_len]\n",
    "            hidden: tuple of (h_0, c_0) or None\n",
    "        Returns:\n",
    "            logits: [batch_size, seq_len, vocab_size]\n",
    "            hidden: tuple of (h_n, c_n)\n",
    "            output: LSTM output [batch_size, seq_len, hidden_dim]\n",
    "        \"\"\"\n",
    "        # Embedding: [batch, seq_len] -> [batch, seq_len, embedding_dim]\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # LSTM: [batch, seq_len, embedding_dim] -> [batch, seq_len, hidden_dim]\n",
    "        if hidden is not None:\n",
    "            lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        else:\n",
    "            lstm_out, hidden = self.lstm(embedded)\n",
    "\n",
    "        # Dropout\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        # Output projection: [batch, seq_len, hidden_dim] -> [batch, seq_len, vocab_size]\n",
    "        logits = self.fc(lstm_out)\n",
    "\n",
    "        return logits, hidden, lstm_out\n",
    "\n",
    "    def init_hidden(self, batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Initialize hidden state\"\"\"\n",
    "        num_directions = 2 if self.config.bidirectional else 1\n",
    "        h0 = torch.zeros(\n",
    "            self.config.num_layers * num_directions,\n",
    "            batch_size,\n",
    "            self.config.hidden_dim,\n",
    "            device=device\n",
    "        )\n",
    "        c0 = torch.zeros(\n",
    "            self.config.num_layers * num_directions,\n",
    "            batch_size,\n",
    "            self.config.hidden_dim,\n",
    "            device=device\n",
    "        )\n",
    "        return (h0, c0)\n",
    "\n",
    "    def get_num_params(self) -> int:\n",
    "        \"\"\"Count total parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL CONFIGURATIONS FOR SCALING STUDY\n",
    "# =============================================================================\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    'tiny': LSTMConfig(\n",
    "        vocab_size=256,\n",
    "        embedding_dim=64,\n",
    "        hidden_dim=128,\n",
    "        num_layers=2,\n",
    "        dropout=0.2\n",
    "    ),\n",
    "    'small': LSTMConfig(\n",
    "        vocab_size=256,\n",
    "        embedding_dim=128,\n",
    "        hidden_dim=256,\n",
    "        num_layers=3,\n",
    "        dropout=0.2\n",
    "    ),\n",
    "    'medium': LSTMConfig(\n",
    "        vocab_size=256,\n",
    "        embedding_dim=256,\n",
    "        hidden_dim=384,\n",
    "        num_layers=4,\n",
    "        dropout=0.3\n",
    "    ),\n",
    "    'large': LSTMConfig(\n",
    "        vocab_size=256,\n",
    "        embedding_dim=384,\n",
    "        hidden_dim=512,\n",
    "        num_layers=5,\n",
    "        dropout=0.3\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET\n",
    "# =============================================================================\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for sequence modeling\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, vocab_path: str, seq_length: int = 256):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path: path to text file\n",
    "            vocab_path: path to vocabulary JSON\n",
    "            seq_length: sequence length for training\n",
    "        \"\"\"\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "        # Load and tokenize data\n",
    "        print(f\"Loading data from {data_path}...\")\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        print(\"Tokenizing...\")\n",
    "        self.data = self._encode(text)\n",
    "        print(f\"Dataset contains {len(self.data):,} tokens\")\n",
    "\n",
    "    def _encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Encode text to indices\"\"\"\n",
    "        unk_idx = self.char_to_idx.get('<UNK>', 0)\n",
    "        return [self.char_to_idx.get(ch, unk_idx) for ch in text]\n",
    "\n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        \"\"\"Decode indices to text\"\"\"\n",
    "        return ''.join([self.idx_to_char.get(idx, '<UNK>') for idx in indices])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get a training example\"\"\"\n",
    "        # Input: sequence of length seq_length\n",
    "        # Target: next token for each position\n",
    "        x = torch.tensor(self.data[idx:idx + self.seq_length], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx + 1:idx + self.seq_length + 1], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training hyperparameters\"\"\"\n",
    "    # Data\n",
    "    data_dir: str = \"/content/drive/MyDrive/processed_music_data\"\n",
    "    vocab_file: str = \"vocab.json\"\n",
    "    seq_length: int = 256\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 64\n",
    "    num_epochs: int = 1\n",
    "    learning_rate: float = 0.001\n",
    "    grad_clip: float = 5.0\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer: str = 'adam'  # 'adam' or 'sgd'\n",
    "    momentum: float = 0.9  # for SGD\n",
    "    weight_decay: float = 1e-5\n",
    "\n",
    "    # Learning rate schedule\n",
    "    use_scheduler: bool = True\n",
    "    scheduler_type: str = 'step'  # 'step' or 'reduce_on_plateau'\n",
    "    step_size: int = 5\n",
    "    gamma: float = 0.5\n",
    "    patience: int = 2\n",
    "\n",
    "    # Evaluation\n",
    "    eval_interval: int = 500\n",
    "\n",
    "    # System\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    num_workers: int = 2\n",
    "\n",
    "    # Output\n",
    "    output_dir: str = \"/content/drive/MyDrive/models/lstm_checkpoints\"\n",
    "    save_best_only: bool = True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(\n",
    "    model: LSTMLanguageModel,\n",
    "    train_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    grad_clip: float,\n",
    "    epoch: int\n",
    ") -> float:\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(pbar):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits, _, _ = model(x)\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        # logits: [batch, seq_len, vocab_size] -> [batch*seq_len, vocab_size]\n",
    "        # y: [batch, seq_len] -> [batch*seq_len]\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: LSTMLanguageModel,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for x, y in val_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, _, _ = model(x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = np.exp(avg_loss)\n",
    "\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model_name: str,\n",
    "    config: LSTMConfig,\n",
    "    train_config: TrainingConfig\n",
    ") -> Dict:\n",
    "    \"\"\"Complete training pipeline\"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {model_name.upper()} Model\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Update vocab size\n",
    "    vocab_path = Path(train_config.data_dir) / train_config.vocab_file\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    config.vocab_size = len(vocab)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = SequenceDataset(\n",
    "        Path(train_config.data_dir) / \"train.txt\",\n",
    "        vocab_path,\n",
    "        train_config.seq_length\n",
    "    )\n",
    "\n",
    "    val_dataset = SequenceDataset(\n",
    "        Path(train_config.data_dir) / \"val.txt\",\n",
    "        vocab_path,\n",
    "        train_config.seq_length\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=train_config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=train_config.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=train_config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=train_config.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Create model\n",
    "    device = torch.device(train_config.device)\n",
    "    model = LSTMLanguageModel(config).to(device)\n",
    "\n",
    "    n_params = model.get_num_params()\n",
    "    print(f\"Model parameters: {n_params:,}\")\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    if train_config.optimizer.lower() == 'adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=train_config.learning_rate,\n",
    "            weight_decay=train_config.weight_decay\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=train_config.learning_rate,\n",
    "            momentum=train_config.momentum,\n",
    "            weight_decay=train_config.weight_decay\n",
    "        )\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = None\n",
    "    if train_config.use_scheduler:\n",
    "        if train_config.scheduler_type == 'step':\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer,\n",
    "                step_size=train_config.step_size,\n",
    "                gamma=train_config.gamma\n",
    "            )\n",
    "        else:\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',\n",
    "                factor=train_config.gamma,\n",
    "                patience=train_config.patience\n",
    "            )\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, train_config.num_epochs + 1):\n",
    "        # Train\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, criterion, optimizer,\n",
    "            device, train_config.grad_clip, epoch\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Evaluate\n",
    "        val_loss, perplexity = evaluate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch}/{train_config.num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "        # Update learning rate\n",
    "        if scheduler is not None:\n",
    "            if train_config.scheduler_type == 'step':\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                scheduler.step(val_loss)\n",
    "            print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            output_dir = Path(train_config.output_dir) / model_name\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'config': asdict(config)\n",
    "            }, output_dir / \"best_model.pt\")\n",
    "            print(f\"Saved best model (val_loss: {val_loss:.4f})\")\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Final evaluation\n",
    "    final_val_loss, final_perplexity = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    # Get GPU memory\n",
    "    gpu_memory = 0\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "\n",
    "    # Results\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'n_params': n_params,\n",
    "        'config': asdict(config),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'final_val_loss': final_val_loss,\n",
    "        'final_perplexity': final_perplexity,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'training_time': training_time,\n",
    "        'gpu_memory_gb': gpu_memory\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    output_dir = Path(train_config.output_dir) / model_name\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(output_dir / \"results.json\", 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    with open(output_dir / \"config.json\", 'w') as f:\n",
    "        json.dump(asdict(config), f, indent=2)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training Complete: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Parameters: {n_params:,}\")\n",
    "    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Final Perplexity: {final_perplexity:.2f}\")\n",
    "    print(f\"Training Time: {training_time/60:.1f} minutes\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.2f} GB\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Train all models\"\"\"\n",
    "\n",
    "    train_config = TrainingConfig(\n",
    "        data_dir=\"/content/drive/MyDrive/processed_music_data\",\n",
    "        batch_size=64,\n",
    "        num_epochs=1,\n",
    "        learning_rate=0.001,\n",
    "        output_dir=\"/content/drive/MyDrive/models/lstm_checkpoints\"\n",
    "    )\n",
    "\n",
    "    print(f\"Device: {train_config.device}\")\n",
    "    print(f\"Data directory: {train_config.data_dir}\")\n",
    "    print(f\"Output directory: {train_config.output_dir}\")\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    for model_name, config in MODEL_CONFIGS.items():\n",
    "        results = train_model(model_name, config, train_config)\n",
    "        all_results[model_name] = results\n",
    "\n",
    "        # Clear cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Save combined results\n",
    "    output_dir = Path(train_config.output_dir)\n",
    "    with open(output_dir / \"all_results.json\", 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training Summary\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Model':<10} {'Params':<12} {'Val Loss':<12} {'Perplexity':<12}\")\n",
    "    print(\"-\"*70)\n",
    "    for name, res in all_results.items():\n",
    "        print(f\"{name:<10} {res['n_params']:>11,} {res['final_val_loss']:>11.4f} {res['final_perplexity']:>11.2f}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1W2T75rXZPv3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RNN (LSTM) vs Transformer (GPT) Scaling Law Comparison\n",
    "Complete analysis of architectural scaling differences\n",
    "\n",
    "CS-GY 6923 Scaling Laws Project\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.optimize import curve_fit\n",
    "from typing import Dict, List, Tuple\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for publication-quality plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# =============================================================================\n",
    "# SCALING LAW FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def power_law(N, a, alpha, c):\n",
    "    \"\"\"Power law: L = a * N^(-alpha) + c\"\"\"\n",
    "    return a * np.power(N, -alpha) + c\n",
    "\n",
    "\n",
    "def fit_scaling_law(param_counts: np.ndarray, losses: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "    \"\"\"Fit power law to scaling data\"\"\"\n",
    "    p0 = [1.0, 0.1, 0.5]\n",
    "\n",
    "    try:\n",
    "        popt, pcov = curve_fit(power_law, param_counts, losses, p0=p0, maxfev=10000)\n",
    "\n",
    "        # Calculate R-squared\n",
    "        residuals = losses - power_law(param_counts, *popt)\n",
    "        ss_res = np.sum(residuals**2)\n",
    "        ss_tot = np.sum((losses - np.mean(losses))**2)\n",
    "        r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "        perr = np.sqrt(np.diag(pcov))\n",
    "\n",
    "        fit_info = {\n",
    "            'a': popt[0],\n",
    "            'alpha': popt[1],\n",
    "            'c': popt[2],\n",
    "            'a_stderr': perr[0],\n",
    "            'alpha_stderr': perr[1],\n",
    "            'c_stderr': perr[2],\n",
    "            'r_squared': r_squared\n",
    "        }\n",
    "\n",
    "        return popt, fit_info\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "def load_results(lstm_file: Path, gpt_file: Path) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"Load results from both architectures\"\"\"\n",
    "\n",
    "    print(\"Loading LSTM results...\")\n",
    "    with open(lstm_file, 'r') as f:\n",
    "        lstm_results = json.load(f)\n",
    "\n",
    "    print(\"Loading GPT results...\")\n",
    "    with open(gpt_file, 'r') as f:\n",
    "        gpt_results = json.load(f)\n",
    "\n",
    "    return lstm_results, gpt_results\n",
    "\n",
    "\n",
    "def extract_metrics(results: Dict) -> Dict:\n",
    "    \"\"\"Extract key metrics from results\"\"\"\n",
    "    params = []\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    times = []\n",
    "    memory = []\n",
    "    model_names = []\n",
    "\n",
    "    for name, res in results.items():\n",
    "        model_names.append(name)\n",
    "        params.append(res['n_params'])\n",
    "\n",
    "        # Handle different key names for validation loss\n",
    "        if 'val_loss' in res:\n",
    "            val_losses.append(res['val_loss'])\n",
    "        elif 'final_val_loss' in res:\n",
    "            val_losses.append(res['final_val_loss'])\n",
    "        else:\n",
    "            val_losses.append(res.get('best_val_loss', 0))\n",
    "\n",
    "        # Handle different key names for training loss\n",
    "        if 'train_loss' in res:\n",
    "            train_losses.append(res['train_loss'])\n",
    "        elif 'train_losses' in res and res['train_losses']:\n",
    "            train_losses.append(res['train_losses'][-1])\n",
    "        else:\n",
    "            train_losses.append(0)\n",
    "\n",
    "        times.append(res.get('training_time', 0))\n",
    "        memory.append(res.get('gpu_memory_gb', 0))\n",
    "\n",
    "    # Sort by parameter count\n",
    "    sort_idx = np.argsort(params)\n",
    "\n",
    "    return {\n",
    "        'model_names': [model_names[i] for i in sort_idx],\n",
    "        'params': np.array([params[i] for i in sort_idx]),\n",
    "        'val_losses': np.array([val_losses[i] for i in sort_idx]),\n",
    "        'train_losses': np.array([train_losses[i] for i in sort_idx]),\n",
    "        'times': np.array([times[i] for i in sort_idx]),\n",
    "        'memory': np.array([memory[i] for i in sort_idx])\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def plot_combined_scaling_laws(lstm_metrics: Dict, gpt_metrics: Dict, output_dir: Path):\n",
    "    \"\"\"Main comparison plot: both scaling curves together\"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # Fit scaling laws\n",
    "    lstm_popt, lstm_fit = fit_scaling_law(lstm_metrics['params'], lstm_metrics['val_losses'])\n",
    "    gpt_popt, gpt_fit = fit_scaling_law(gpt_metrics['params'], gpt_metrics['val_losses'])\n",
    "\n",
    "    # Plot LSTM\n",
    "    ax.scatter(lstm_metrics['params'], lstm_metrics['val_losses'],\n",
    "              s=150, alpha=0.7, c='#E74C3C', edgecolors='black',\n",
    "              linewidth=1.5, marker='s', label='LSTM (observed)', zorder=3)\n",
    "\n",
    "    for name, n, l in zip(lstm_metrics['model_names'], lstm_metrics['params'], lstm_metrics['val_losses']):\n",
    "        ax.annotate(f\"LSTM-{name}\", (n, l), xytext=(8, -8),\n",
    "                   textcoords='offset points', fontsize=8, alpha=0.7)\n",
    "\n",
    "    # Plot GPT\n",
    "    ax.scatter(gpt_metrics['params'], gpt_metrics['val_losses'],\n",
    "              s=150, alpha=0.7, c='#3498DB', edgecolors='black',\n",
    "              linewidth=1.5, marker='o', label='Transformer (observed)', zorder=3)\n",
    "\n",
    "    for name, n, l in zip(gpt_metrics['model_names'], gpt_metrics['params'], gpt_metrics['val_losses']):\n",
    "        ax.annotate(f\"GPT-{name}\", (n, l), xytext=(8, 8),\n",
    "                   textcoords='offset points', fontsize=8, alpha=0.7)\n",
    "\n",
    "    # Plot fitted curves\n",
    "    if lstm_popt is not None:\n",
    "        N_smooth = np.logspace(np.log10(lstm_metrics['params'].min()),\n",
    "                               np.log10(lstm_metrics['params'].max()), 100)\n",
    "        L_fitted = power_law(N_smooth, *lstm_popt)\n",
    "        ax.plot(N_smooth, L_fitted, '--', linewidth=2.5, color='#E74C3C',\n",
    "                label=f'LSTM fit (α={lstm_fit[\"alpha\"]:.4f})', alpha=0.8, zorder=2)\n",
    "\n",
    "    if gpt_popt is not None:\n",
    "        N_smooth = np.logspace(np.log10(gpt_metrics['params'].min()),\n",
    "                               np.log10(gpt_metrics['params'].max()), 100)\n",
    "        L_fitted = power_law(N_smooth, *gpt_popt)\n",
    "        ax.plot(N_smooth, L_fitted, '--', linewidth=2.5, color='#3498DB',\n",
    "                label=f'Transformer fit (α={gpt_fit[\"alpha\"]:.4f})', alpha=0.8, zorder=2)\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Number of Parameters (N)', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Validation Loss (L)', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Scaling Laws: LSTM vs Transformer Architecture',\n",
    "                 fontsize=15, fontweight='bold', pad=20)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.legend(fontsize=10, loc='upper right', framealpha=0.9)\n",
    "\n",
    "    # Add comparison box\n",
    "    if lstm_fit and gpt_fit:\n",
    "        comparison_text = (\n",
    "            f\"Scaling Exponents:\\n\"\n",
    "            f\"LSTM: α = {lstm_fit['alpha']:.4f} ± {lstm_fit['alpha_stderr']:.4f}\\n\"\n",
    "            f\"Transformer: α = {gpt_fit['alpha']:.4f} ± {gpt_fit['alpha_stderr']:.4f}\\n\"\n",
    "            f\"\\n\"\n",
    "            f\"R² Values:\\n\"\n",
    "            f\"LSTM: {lstm_fit['r_squared']:.4f}\\n\"\n",
    "            f\"Transformer: {gpt_fit['r_squared']:.4f}\\n\"\n",
    "            f\"\\n\"\n",
    "            f\"{'Transformer scales better!' if gpt_fit['alpha'] > lstm_fit['alpha'] else 'LSTM scales better!'}\"\n",
    "        )\n",
    "\n",
    "        ax.text(0.03, 0.97, comparison_text, transform=ax.transAxes,\n",
    "                fontsize=9, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.85))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'combined_scaling_laws.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved combined scaling plot to {output_dir / 'combined_scaling_laws.png'}\")\n",
    "\n",
    "    return fig, lstm_fit, gpt_fit\n",
    "\n",
    "\n",
    "def plot_computational_efficiency(lstm_metrics: Dict, gpt_metrics: Dict, output_dir: Path):\n",
    "    \"\"\"Compare computational efficiency: time and memory per parameter\"\"\"\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    lstm_time_per_param = lstm_metrics['times'] / lstm_metrics['params'] * 1e6  # microseconds per param\n",
    "    gpt_time_per_param = gpt_metrics['times'] / gpt_metrics['params'] * 1e6\n",
    "\n",
    "    lstm_memory_per_param = lstm_metrics['memory'] / lstm_metrics['params'] * 1e9  # bytes per param\n",
    "    gpt_memory_per_param = gpt_metrics['memory'] / gpt_metrics['params'] * 1e9\n",
    "\n",
    "    # Plot 1: Training time per parameter\n",
    "    ax1.scatter(lstm_metrics['params'], lstm_time_per_param,\n",
    "               s=150, alpha=0.7, c='#E74C3C', edgecolors='black',\n",
    "               linewidth=1.5, marker='s', label='LSTM', zorder=3)\n",
    "    ax1.scatter(gpt_metrics['params'], gpt_time_per_param,\n",
    "               s=150, alpha=0.7, c='#3498DB', edgecolors='black',\n",
    "               linewidth=1.5, marker='o', label='Transformer', zorder=3)\n",
    "\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_xlabel('Number of Parameters', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Training Time per Parameter (μs)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Time Efficiency: Training Time per Parameter', fontsize=13, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax1.legend(fontsize=10)\n",
    "\n",
    "    # Add annotations\n",
    "    for name, n, t in zip(lstm_metrics['model_names'], lstm_metrics['params'], lstm_time_per_param):\n",
    "        ax1.annotate(name, (n, t), xytext=(5, 5), textcoords='offset points', fontsize=8, alpha=0.7)\n",
    "    for name, n, t in zip(gpt_metrics['model_names'], gpt_metrics['params'], gpt_time_per_param):\n",
    "        ax1.annotate(name, (n, t), xytext=(5, -10), textcoords='offset points', fontsize=8, alpha=0.7)\n",
    "\n",
    "    # Plot 2: Memory per parameter\n",
    "    if any(lstm_memory_per_param > 0) and any(gpt_memory_per_param > 0):\n",
    "        ax2.scatter(lstm_metrics['params'], lstm_memory_per_param,\n",
    "                   s=150, alpha=0.7, c='#E74C3C', edgecolors='black',\n",
    "                   linewidth=1.5, marker='s', label='LSTM', zorder=3)\n",
    "        ax2.scatter(gpt_metrics['params'], gpt_memory_per_param,\n",
    "                   s=150, alpha=0.7, c='#3498DB', edgecolors='black',\n",
    "                   linewidth=1.5, marker='o', label='Transformer', zorder=3)\n",
    "\n",
    "        ax2.set_xscale('log')\n",
    "        ax2.set_xlabel('Number of Parameters', fontsize=12, fontweight='bold')\n",
    "        ax2.set_ylabel('Memory per Parameter (bytes)', fontsize=12, fontweight='bold')\n",
    "        ax2.set_title('Memory Efficiency: GPU Memory per Parameter', fontsize=13, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax2.legend(fontsize=10)\n",
    "\n",
    "        # Add annotations\n",
    "        for name, n, m in zip(lstm_metrics['model_names'], lstm_metrics['params'], lstm_memory_per_param):\n",
    "            ax2.annotate(name, (n, m), xytext=(5, 5), textcoords='offset points', fontsize=8, alpha=0.7)\n",
    "        for name, n, m in zip(gpt_metrics['model_names'], gpt_metrics['params'], gpt_memory_per_param):\n",
    "            ax2.annotate(name, (n, m), xytext=(5, -10), textcoords='offset points', fontsize=8, alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'computational_efficiency.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved computational efficiency plot to {output_dir / 'computational_efficiency.png'}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_absolute_performance(lstm_metrics: Dict, gpt_metrics: Dict, output_dir: Path):\n",
    "    \"\"\"Compare absolute training time and memory usage\"\"\"\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot 1: Total training time\n",
    "    ax1.scatter(lstm_metrics['params'], lstm_metrics['times'] / 60,\n",
    "               s=150, alpha=0.7, c='#E74C3C', edgecolors='black',\n",
    "               linewidth=1.5, marker='s', label='LSTM', zorder=3)\n",
    "    ax1.scatter(gpt_metrics['params'], gpt_metrics['times'] / 60,\n",
    "               s=150, alpha=0.7, c='#3498DB', edgecolors='black',\n",
    "               linewidth=1.5, marker='o', label='Transformer', zorder=3)\n",
    "\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_xlabel('Number of Parameters', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Total Training Time (minutes)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Absolute Training Time vs Model Size', fontsize=13, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax1.legend(fontsize=10)\n",
    "\n",
    "    # Add annotations\n",
    "    for name, n, t in zip(lstm_metrics['model_names'], lstm_metrics['params'], lstm_metrics['times'] / 60):\n",
    "        ax1.annotate(name, (n, t), xytext=(5, 5), textcoords='offset points', fontsize=8, alpha=0.7)\n",
    "    for name, n, t in zip(gpt_metrics['model_names'], gpt_metrics['params'], gpt_metrics['times'] / 60):\n",
    "        ax1.annotate(name, (n, t), xytext=(5, -10), textcoords='offset points', fontsize=8, alpha=0.7)\n",
    "\n",
    "    # Plot 2: Total GPU memory\n",
    "    if any(lstm_metrics['memory'] > 0) and any(gpt_metrics['memory'] > 0):\n",
    "        ax2.scatter(lstm_metrics['params'], lstm_metrics['memory'],\n",
    "                   s=150, alpha=0.7, c='#E74C3C', edgecolors='black',\n",
    "                   linewidth=1.5, marker='s', label='LSTM', zorder=3)\n",
    "        ax2.scatter(gpt_metrics['params'], gpt_metrics['memory'],\n",
    "                   s=150, alpha=0.7, c='#3498DB', edgecolors='black',\n",
    "                   linewidth=1.5, marker='o', label='Transformer', zorder=3)\n",
    "\n",
    "        ax2.set_xscale('log')\n",
    "        ax2.set_xlabel('Number of Parameters', fontsize=12, fontweight='bold')\n",
    "        ax2.set_ylabel('Peak GPU Memory (GB)', fontsize=12, fontweight='bold')\n",
    "        ax2.set_title('Absolute GPU Memory vs Model Size', fontsize=13, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax2.legend(fontsize=10)\n",
    "\n",
    "        # Add annotations\n",
    "        for name, n, m in zip(lstm_metrics['model_names'], lstm_metrics['params'], lstm_metrics['memory']):\n",
    "            ax2.annotate(name, (n, m), xytext=(5, 5), textcoords='offset points', fontsize=8, alpha=0.7)\n",
    "        for name, n, m in zip(gpt_metrics['model_names'], gpt_metrics['params'], gpt_metrics['memory']):\n",
    "            ax2.annotate(name, (n, m), xytext=(5, -10), textcoords='offset points', fontsize=8, alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'absolute_resources.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved absolute resources plot to {output_dir / 'absolute_resources.png'}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_sample_efficiency(lstm_metrics: Dict, gpt_metrics: Dict, output_dir: Path):\n",
    "    \"\"\"Compare sample efficiency: validation loss achieved vs training time\"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "    # Plot loss vs training time\n",
    "    ax.scatter(lstm_metrics['times'] / 60, lstm_metrics['val_losses'],\n",
    "              s=200, alpha=0.7, c='#E74C3C', edgecolors='black',\n",
    "              linewidth=1.5, marker='s', label='LSTM', zorder=3)\n",
    "    ax.scatter(gpt_metrics['times'] / 60, gpt_metrics['val_losses'],\n",
    "              s=200, alpha=0.7, c='#3498DB', edgecolors='black',\n",
    "              linewidth=1.5, marker='o', label='Transformer', zorder=3)\n",
    "\n",
    "    # Add model size as labels\n",
    "    for name, t, l, p in zip(lstm_metrics['model_names'], lstm_metrics['times'] / 60,\n",
    "                             lstm_metrics['val_losses'], lstm_metrics['params']):\n",
    "        ax.annotate(f\"{name}\\n({p:,})\", (t, l), xytext=(8, 8),\n",
    "                   textcoords='offset points', fontsize=8, alpha=0.8)\n",
    "\n",
    "    for name, t, l, p in zip(gpt_metrics['model_names'], gpt_metrics['times'] / 60,\n",
    "                            gpt_metrics['val_losses'], gpt_metrics['params']):\n",
    "        ax.annotate(f\"{name}\\n({p:,})\", (t, l), xytext=(8, -12),\n",
    "                   textcoords='offset points', fontsize=8, alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel('Training Time (minutes)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Validation Loss', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Sample Efficiency: Loss vs Training Time\\n(Lower-left is better)',\n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.legend(fontsize=11, loc='best')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'sample_efficiency.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved sample efficiency plot to {output_dir / 'sample_efficiency.png'}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_loss_comparison_bar(lstm_metrics: Dict, gpt_metrics: Dict, output_dir: Path):\n",
    "    \"\"\"Side-by-side comparison of validation losses\"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # Combine data\n",
    "    n_lstm = len(lstm_metrics['model_names'])\n",
    "    n_gpt = len(gpt_metrics['model_names'])\n",
    "\n",
    "    x = np.arange(max(n_lstm, n_gpt))\n",
    "    width = 0.35\n",
    "\n",
    "    # Create bars\n",
    "    lstm_bars = ax.bar(x[:n_lstm] - width/2, lstm_metrics['val_losses'], width,\n",
    "                       label='LSTM', color='#E74C3C', edgecolor='black', linewidth=1.2)\n",
    "    gpt_bars = ax.bar(x[:n_gpt] + width/2, gpt_metrics['val_losses'], width,\n",
    "                      label='Transformer', color='#3498DB', edgecolor='black', linewidth=1.2)\n",
    "\n",
    "    # Add value labels\n",
    "    for bars in [lstm_bars, gpt_bars]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlabel('Model Size Category', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Validation Loss', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Validation Loss Comparison: LSTM vs Transformer',\n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x)\n",
    "\n",
    "    # Create labels with parameter counts\n",
    "    labels = []\n",
    "    for i in range(max(n_lstm, n_gpt)):\n",
    "        if i < n_lstm and i < n_gpt:\n",
    "            label = f\"{lstm_metrics['model_names'][i]}/{gpt_metrics['model_names'][i]}\\n\"\n",
    "            label += f\"({lstm_metrics['params'][i]:,} / {gpt_metrics['params'][i]:,})\"\n",
    "        elif i < n_lstm:\n",
    "            label = f\"{lstm_metrics['model_names'][i]}\\n({lstm_metrics['params'][i]:,})\"\n",
    "        else:\n",
    "            label = f\"{gpt_metrics['model_names'][i]}\\n({gpt_metrics['params'][i]:,})\"\n",
    "        labels.append(label)\n",
    "\n",
    "    ax.set_xticklabels(labels, fontsize=9)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'loss_comparison_bars.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved loss comparison bars to {output_dir / 'loss_comparison_bars.png'}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ANALYSIS AND REPORTING\n",
    "# =============================================================================\n",
    "\n",
    "def generate_comparison_report(lstm_fit: Dict, gpt_fit: Dict,\n",
    "                              lstm_metrics: Dict, gpt_metrics: Dict,\n",
    "                              output_dir: Path):\n",
    "    \"\"\"Generate comprehensive comparison report\"\"\"\n",
    "\n",
    "    report = []\n",
    "    report.append(\"=\"*80)\n",
    "    report.append(\"LSTM VS TRANSFORMER SCALING LAW COMPARISON\")\n",
    "    report.append(\"=\"*80)\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Scaling law comparison\n",
    "    report.append(\"1. SCALING LAW PARAMETERS\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(f\"{'Architecture':<15} {'α (exponent)':<20} {'a (coeff)':<20} {'c (offset)':<15}\")\n",
    "    report.append(\"-\" * 80)\n",
    "\n",
    "    if lstm_fit:\n",
    "        report.append(f\"{'LSTM':<15} {lstm_fit['alpha']:.6f} ± {lstm_fit['alpha_stderr']:.6f}  \"\n",
    "                     f\"{lstm_fit['a']:.6f} ± {lstm_fit['a_stderr']:.6f}  {lstm_fit['c']:.6f}\")\n",
    "\n",
    "    if gpt_fit:\n",
    "        report.append(f\"{'Transformer':<15} {gpt_fit['alpha']:.6f} ± {gpt_fit['alpha_stderr']:.6f}  \"\n",
    "                     f\"{gpt_fit['a']:.6f} ± {gpt_fit['a_stderr']:.6f}  {gpt_fit['c']:.6f}\")\n",
    "\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Interpretation\n",
    "    report.append(\"2. SCALING EFFICIENCY ANALYSIS\")\n",
    "    report.append(\"-\" * 80)\n",
    "\n",
    "    if lstm_fit and gpt_fit:\n",
    "        alpha_diff = gpt_fit['alpha'] - lstm_fit['alpha']\n",
    "        better_arch = \"Transformer\" if alpha_diff > 0 else \"LSTM\"\n",
    "\n",
    "        report.append(f\"Scaling Exponent Difference: Δα = {alpha_diff:.6f}\")\n",
    "        report.append(f\"Better Scaling Architecture: {better_arch}\")\n",
    "        report.append(\"\")\n",
    "\n",
    "        # Calculate parameter multipliers for same loss reduction\n",
    "        lstm_mult = 2 ** (1 / lstm_fit['alpha'])\n",
    "        gpt_mult = 2 ** (1 / gpt_fit['alpha'])\n",
    "\n",
    "        report.append(f\"To halve the loss:\")\n",
    "        report.append(f\"  LSTM needs:        {lstm_mult:.2f}× more parameters\")\n",
    "        report.append(f\"  Transformer needs: {gpt_mult:.2f}× more parameters\")\n",
    "        report.append(f\"  Efficiency ratio:  {lstm_mult / gpt_mult:.2f}× (Transformer advantage)\")\n",
    "        report.append(\"\")\n",
    "\n",
    "    # Computational efficiency\n",
    "    report.append(\"3. COMPUTATIONAL EFFICIENCY\")\n",
    "    report.append(\"-\" * 80)\n",
    "\n",
    "    lstm_avg_time_per_param = np.mean(lstm_metrics['times'] / lstm_metrics['params'])\n",
    "    gpt_avg_time_per_param = np.mean(gpt_metrics['times'] / gpt_metrics['params'])\n",
    "\n",
    "    lstm_avg_mem_per_param = np.mean(lstm_metrics['memory'] / lstm_metrics['params']) if any(lstm_metrics['memory'] > 0) else 0\n",
    "    gpt_avg_mem_per_param = np.mean(gpt_metrics['memory'] / gpt_metrics['params']) if any(gpt_metrics['memory'] > 0) else 0\n",
    "\n",
    "    report.append(f\"Average Training Time per Parameter:\")\n",
    "    report.append(f\"  LSTM:        {lstm_avg_time_per_param*1e6:.2f} μs/param\")\n",
    "    report.append(f\"  Transformer: {gpt_avg_time_per_param*1e6:.2f} μs/param\")\n",
    "    report.append(f\"  Ratio:       {lstm_avg_time_per_param / gpt_avg_time_per_param:.2f}× \"\n",
    "                 f\"({'LSTM faster' if lstm_avg_time_per_param < gpt_avg_time_per_param else 'Transformer faster'})\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    if lstm_avg_mem_per_param > 0 and gpt_avg_mem_per_param > 0:\n",
    "        report.append(f\"Average GPU Memory per Parameter:\")\n",
    "        report.append(f\"  LSTM:        {lstm_avg_mem_per_param*1e9:.2f} bytes/param\")\n",
    "        report.append(f\"  Transformer: {gpt_avg_mem_per_param*1e9:.2f} bytes/param\")\n",
    "        report.append(f\"  Ratio:       {lstm_avg_mem_per_param / gpt_avg_mem_per_param:.2f}× \"\n",
    "                     f\"({'LSTM more efficient' if lstm_avg_mem_per_param < gpt_avg_mem_per_param else 'Transformer more efficient'})\")\n",
    "        report.append(\"\")\n",
    "\n",
    "    # Sample efficiency\n",
    "    report.append(\"4. SAMPLE EFFICIENCY\")\n",
    "    report.append(\"-\" * 80)\n",
    "\n",
    "    # Best loss per architecture\n",
    "    lstm_best_idx = np.argmin(lstm_metrics['val_losses'])\n",
    "    gpt_best_idx = np.argmin(gpt_metrics['val_losses'])\n",
    "\n",
    "    report.append(f\"Best Validation Loss:\")\n",
    "    report.append(f\"  LSTM:        {lstm_metrics['val_losses'][lstm_best_idx]:.6f} \"\n",
    "                 f\"({lstm_metrics['model_names'][lstm_best_idx]}, {lstm_metrics['params'][lstm_best_idx]:,} params)\")\n",
    "    report.append(f\"  Transformer: {gpt_metrics['val_losses'][gpt_best_idx]:.6f} \"\n",
    "                 f\"({gpt_metrics['model_names'][gpt_best_idx]}, {gpt_metrics['params'][gpt_best_idx]:,} params)\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Loss per training time\n",
    "    lstm_loss_per_time = lstm_metrics['val_losses'] / (lstm_metrics['times'] / 3600)  # loss per hour\n",
    "    gpt_loss_per_time = gpt_metrics['val_losses'] / (gpt_metrics['times'] / 3600)\n",
    "\n",
    "    report.append(f\"Loss Achieved per Training Hour (lower is better):\")\n",
    "    report.append(f\"  LSTM average:        {np.mean(lstm_loss_per_time):.6f}\")\n",
    "    report.append(f\"  Transformer average: {np.mean(gpt_loss_per_time):.6f}\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Discussion\n",
    "    report.append(\"5. DISCUSSION: WHY THESE DIFFERENCES?\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(\"\")\n",
    "    report.append(\"Architectural Differences:\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"a) Parallelization:\")\n",
    "    report.append(\"   - Transformers: Process entire sequence in parallel (attention)\")\n",
    "    report.append(\"   - LSTMs: Sequential processing (one token at a time)\")\n",
    "    report.append(\"   → Transformers train faster per epoch but may use more memory\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"b) Long-range Dependencies:\")\n",
    "    report.append(\"   - Transformers: Direct connections via self-attention (O(n²))\")\n",
    "    report.append(\"   - LSTMs: Information flows through hidden state (sequential)\")\n",
    "    report.append(\"   → Transformers better at capturing long-range patterns\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"c) Parameter Efficiency:\")\n",
    "\n",
    "    gpt_alpha = f\"{gpt_fit['alpha']:.4f}\" if gpt_fit else \"N/A\"\n",
    "    lstm_alpha = f\"{lstm_fit['alpha']:.4f}\" if lstm_fit else \"N/A\"\n",
    "\n",
    "    report.append(f\"   - Higher α value ({gpt_alpha} vs {lstm_alpha}) means:\")\n",
    "\n",
    "    if lstm_fit and gpt_fit:\n",
    "        if gpt_fit['alpha'] > lstm_fit['alpha']:\n",
    "            report.append(\"   - Transformers scale better: more benefit from additional parameters\")\n",
    "            report.append(\"   - Each new parameter in Transformer provides more loss reduction\")\n",
    "        else:\n",
    "            report.append(\"   - LSTMs scale better: more benefit from additional parameters\")\n",
    "            report.append(\"   - Each new parameter in LSTM provides more loss reduction\")\n",
    "\n",
    "    report.append(\"\")\n",
    "    report.append(\"d) Computational Trade-offs:\")\n",
    "    report.append(\"   - Transformers: Higher throughput (parallel), higher memory (attention)\")\n",
    "    report.append(\"   - LSTMs: Lower memory, sequential bottleneck limits speed\")\n",
    "    report.append(\"\")\n",
    "\n",
    "    # Recommendations\n",
    "    report.append(\"6. PRACTICAL RECOMMENDATIONS\")\n",
    "    report.append(\"-\" * 80)\n",
    "\n",
    "    if lstm_fit and gpt_fit and gpt_fit['alpha'] > lstm_fit['alpha']:\n",
    "        report.append(\"Choose TRANSFORMERS when:\")\n",
    "        report.append(\"  - Scaling to large models (better parameter efficiency)\")\n",
    "        report.append(\"  - GPU memory is available (supports parallel attention)\")\n",
    "        report.append(\"  - Long-range dependencies are important\")\n",
    "        report.append(\"  - Training throughput is priority\")\n",
    "        report.append(\"\")\n",
    "        report.append(\"Choose LSTMS when:\")\n",
    "        report.append(\"  - Resource-constrained environments (lower memory)\")\n",
    "        report.append(\"  - Smaller models are sufficient\")\n",
    "        report.append(\"  - Sequential processing is acceptable\")\n",
    "        report.append(\"  - Inference latency matters more than throughput\")\n",
    "    else:\n",
    "        report.append(\"Based on these results, consider your specific constraints\")\n",
    "        report.append(\"  and requirements when choosing architecture.\")\n",
    "\n",
    "    report.append(\"\")\n",
    "    report.append(\"=\"*80)\n",
    "    report.append(\"END OF REPORT\")\n",
    "    report.append(\"=\"*80)\n",
    "\n",
    "    # Print to console\n",
    "    report_text = \"\\n\".join(report)\n",
    "    print(\"\\n\" + report_text)\n",
    "\n",
    "    # Save to file\n",
    "    with open(output_dir / \"comparison_report.txt\", 'w') as f:\n",
    "        f.write(report_text)\n",
    "\n",
    "    print(f\"\\nReport saved to {output_dir / 'comparison_report.txt'}\")\n",
    "\n",
    "\n",
    "def create_summary_table(lstm_metrics: Dict, gpt_metrics: Dict):\n",
    "    \"\"\"Print summary table\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"DETAILED MODEL COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Architecture':<12} {'Model':<10} {'Params':<15} {'Val Loss':<12} {'Time (min)':<12} {'Memory (GB)':<12}\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    for name, p, l, t, m in zip(lstm_metrics['model_names'], lstm_metrics['params'],\n",
    "                                 lstm_metrics['val_losses'], lstm_metrics['times'] / 60,\n",
    "                                 lstm_metrics['memory']):\n",
    "        print(f\"{'LSTM':<12} {name:<10} {p:>14,} {l:>11.6f} {t:>11.2f} {m:>11.2f}\")\n",
    "\n",
    "    for name, p, l, t, m in zip(gpt_metrics['model_names'], gpt_metrics['params'],\n",
    "                                 gpt_metrics['val_losses'], gpt_metrics['times'] / 60,\n",
    "                                 gpt_metrics['memory']):\n",
    "        print(f\"{'Transformer':<12} {name:<10} {p:>14,} {l:>11.6f} {t:>11.2f} {m:>11.2f}\")\n",
    "\n",
    "    print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run complete comparison analysis\"\"\"\n",
    "\n",
    "    # Configuration\n",
    "    lstm_results_file = Path(\"/content/drive/MyDrive/models/lstm_checkpoints/all_results.json\")\n",
    "    gpt_results_file = Path(\"/content/drive/MyDrive/models/checkpoints/all_results.json\")\n",
    "    output_dir = Path(\"/content/drive/MyDrive/models/comparison_analysis\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"RNN (LSTM) VS TRANSFORMER (GPT) SCALING ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"LSTM results: {lstm_results_file}\")\n",
    "    print(f\"GPT results:  {gpt_results_file}\")\n",
    "    print(f\"Output dir:   {output_dir}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Load data\n",
    "    lstm_results, gpt_results = load_results(lstm_results_file, gpt_results_file)\n",
    "\n",
    "    print(f\"Loaded {len(lstm_results)} LSTM models\")\n",
    "    print(f\"Loaded {len(gpt_results)} GPT models\\n\")\n",
    "\n",
    "    # Extract metrics\n",
    "    lstm_metrics = extract_metrics(lstm_results)\n",
    "    gpt_metrics = extract_metrics(gpt_results)\n",
    "\n",
    "    # Print summary table\n",
    "    create_summary_table(lstm_metrics, gpt_metrics)\n",
    "\n",
    "    # Generate all plots\n",
    "    print(\"\\nGenerating comparison plots...\\n\")\n",
    "\n",
    "    # 1. Combined scaling laws (main plot)\n",
    "    fig1, lstm_fit, gpt_fit = plot_combined_scaling_laws(lstm_metrics, gpt_metrics, output_dir)\n",
    "\n",
    "    # 2. Computational efficiency\n",
    "    fig2 = plot_computational_efficiency(lstm_metrics, gpt_metrics, output_dir)\n",
    "\n",
    "    # 3. Absolute resources\n",
    "    fig3 = plot_absolute_performance(lstm_metrics, gpt_metrics, output_dir)\n",
    "\n",
    "    # 4. Sample efficiency\n",
    "    fig4 = plot_sample_efficiency(lstm_metrics, gpt_metrics, output_dir)\n",
    "\n",
    "    # 5. Loss comparison bars\n",
    "    fig5 = plot_loss_comparison_bar(lstm_metrics, gpt_metrics, output_dir)\n",
    "\n",
    "    # Generate comprehensive report\n",
    "    generate_comparison_report(lstm_fit, gpt_fit, lstm_metrics, gpt_metrics, output_dir)\n",
    "\n",
    "    # Save comparison data\n",
    "    comparison_data = {\n",
    "        'lstm_fit': lstm_fit,\n",
    "        'gpt_fit': gpt_fit,\n",
    "        'lstm_summary': {\n",
    "            'models': lstm_metrics['model_names'],\n",
    "            'params': lstm_metrics['params'].tolist(),\n",
    "            'val_losses': lstm_metrics['val_losses'].tolist(),\n",
    "            'times': lstm_metrics['times'].tolist(),\n",
    "            'memory': lstm_metrics['memory'].tolist()\n",
    "        },\n",
    "        'gpt_summary': {\n",
    "            'models': gpt_metrics['model_names'],\n",
    "            'params': gpt_metrics['params'].tolist(),\n",
    "            'val_losses': gpt_metrics['val_losses'].tolist(),\n",
    "            'times': gpt_metrics['times'].tolist(),\n",
    "            'memory': gpt_metrics['memory'].tolist()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(output_dir / 'comparison_data.json', 'w') as f:\n",
    "        json.dump(comparison_data, f, indent=2)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"All plots and reports saved to: {output_dir}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNFLVWErHbFkzNRW+hcuF3S",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
