{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VWW1V9mhqxe"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete MIDI to ABC Preprocessing Pipeline for Google Colab\n",
    "CS-GY 6923 Scaling Laws Project\n",
    "\n",
    "Implements:\n",
    "Incremental storage inside processed_music_data/raw_abc.txt\n",
    "Checkpoint tracks processed files\n",
    "Multiple runs append new data without overwriting\n",
    "Final dataset creation is separate\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Optional\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "from music21 import converter\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PREPROCESSING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "class MusicDataPreprocessor:\n",
    "    \"\"\"MIDI → ABC converter WITH INCREMENTAL STORAGE\"\"\"\n",
    "\n",
    "    def __init__(self, midi_dir: str, output_dir: str, min_length: int, max_length: int):\n",
    "        self.midi_dir = Path(midi_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # NEW: Raw storage file for incremental writes\n",
    "        self.raw_output_file = self.output_dir / \"raw_abc.txt\"\n",
    "\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.stats = {\n",
    "            'total_files': 0,\n",
    "            'successful_conversions': 0,\n",
    "            'failed_conversions': 0,\n",
    "            'too_short': 0,\n",
    "            'truncated': 0,\n",
    "            'empty_output': 0,\n",
    "            'total_tokens': 0,\n",
    "            'processing_time': 0\n",
    "        }\n",
    "\n",
    "        # Checkpoint file\n",
    "        self.checkpoint_file = self.output_dir / \"checkpoint.json\"\n",
    "        self.processed_files = self._load_checkpoint()\n",
    "\n",
    "    def _load_checkpoint(self) -> set:\n",
    "        if self.checkpoint_file.exists():\n",
    "            with open(self.checkpoint_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                processed = set(data.get('processed_files', []))\n",
    "                if processed:\n",
    "                    print(f\"Loaded checkpoint: {len(processed)} files already processed\")\n",
    "                return processed\n",
    "        return set()\n",
    "\n",
    "    def _save_checkpoint(self):\n",
    "        try:\n",
    "            with open(self.checkpoint_file, 'w') as f:\n",
    "                json.dump({\n",
    "                    'processed_files': list(self.processed_files),\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'stats': self.stats\n",
    "                }, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not save checkpoint: {e}\")\n",
    "\n",
    "    def midi_to_abc(self, midi_path: Path) -> Optional[str]:\n",
    "        try:\n",
    "            score = converter.parse(str(midi_path))\n",
    "            \n",
    "            # Create temporary ABC file\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.abc',\n",
    "                                            delete=False, encoding='utf-8') as tmp:\n",
    "                tmp_path = tmp.name\n",
    "            \n",
    "            try:\n",
    "                # Write score to ABC format\n",
    "                score.write('abc', fp=tmp_path)\n",
    "                \n",
    "                # Read back the ABC text\n",
    "                with open(tmp_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    abc_text = f.read()\n",
    "                \n",
    "                # CRITICAL FIX: Validate it's actual ABC notation, not object representation\n",
    "                if abc_text.strip().startswith('<music21.') or 'object at 0x' in abc_text:\n",
    "                    return None  # Invalid conversion, reject it\n",
    "                \n",
    "                # Additional validation: check for ABC headers\n",
    "                if not any(header in abc_text for header in ['X:', 'T:', 'M:', 'K:']):\n",
    "                    return None  # Doesn't look like valid ABC notation\n",
    "                    \n",
    "                return abc_text\n",
    "                \n",
    "            finally:\n",
    "                if os.path.exists(tmp_path):\n",
    "                    os.unlink(tmp_path)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {midi_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_single_file(self, midi_path: Path) -> Optional[str]:\n",
    "        if str(midi_path) in self.processed_files:\n",
    "            return None\n",
    "\n",
    "        abc_string = self.midi_to_abc(midi_path)\n",
    "\n",
    "        if abc_string is None:\n",
    "            self.stats['failed_conversions'] += 1\n",
    "            return None\n",
    "        if len(abc_string.strip()) == 0:\n",
    "            self.stats['empty_output'] += 1\n",
    "            return None\n",
    "        if len(abc_string) < self.min_length:\n",
    "            self.stats['too_short'] += 1\n",
    "            return None\n",
    "        if len(abc_string) > self.max_length:\n",
    "            abc_string = abc_string[:self.max_length]\n",
    "            self.stats['truncated'] += 1\n",
    "\n",
    "        self.stats['successful_conversions'] += 1\n",
    "        self.stats['total_tokens'] += len(abc_string)\n",
    "\n",
    "        # Mark as processed\n",
    "        self.processed_files.add(str(midi_path))\n",
    "\n",
    "        return abc_string\n",
    "\n",
    "    def process_dataset(self, max_files: Optional[int], save_checkpoint_every: int):\n",
    "        start = time.time()\n",
    "\n",
    "        print(\"Finding MIDI files...\")\n",
    "        midi_files = list(self.midi_dir.rglob(\"*.mid\")) + list(self.midi_dir.rglob(\"*.midi\"))\n",
    "\n",
    "        if max_files:\n",
    "            midi_files = midi_files[:max_files]\n",
    "\n",
    "        midi_files = [f for f in midi_files if str(f) not in self.processed_files]\n",
    "\n",
    "        print(f\"Processing {len(midi_files)} new files\")\n",
    "\n",
    "        # OPEN RAW FILE IN APPEND MODE\n",
    "        with open(self.raw_output_file, 'a', encoding='utf-8') as raw_f:\n",
    "\n",
    "            for i, midi_file in enumerate(tqdm(midi_files)):\n",
    "                abc = self.process_single_file(midi_file)\n",
    "\n",
    "                if abc is not None:\n",
    "                    # Write incrementally\n",
    "                    raw_f.write(abc + \"\\n<|endoftext|>\\n\")\n",
    "\n",
    "                # Save checkpoint periodically\n",
    "                if (i + 1) % save_checkpoint_every == 0:\n",
    "                    self._save_checkpoint()\n",
    "\n",
    "        self._save_checkpoint()\n",
    "\n",
    "        self.stats['processing_time'] += (time.time() - start)\n",
    "\n",
    "        print(\"\\nIncremental processing complete!\")\n",
    "        print(f\"Raw ABC stored at: {self.raw_output_file}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIG\n",
    "# =============================================================================\n",
    "\n",
    "MIDI_DIR = \"/content/drive/MyDrive/lmd_full/lmd_full\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/processed_music_data\"\n",
    "\n",
    "MAX_FILES = 5000\n",
    "MIN_LENGTH = 10\n",
    "MAX_LENGTH = 4096\n",
    "TOKENIZATION = \"character\"\n",
    "\n",
    "TRAIN_RATIO = 0.98\n",
    "VAL_RATIO = 0.01\n",
    "SEED = 42\n",
    "\n",
    "SAVE_CHECKPOINT_EVERY = 1000\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"\\n=== INCREMENTAL MIDI → ABC PREPROCESSOR ===\\n\")\n",
    "\n",
    "    pre = MusicDataPreprocessor(\n",
    "        midi_dir=MIDI_DIR,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        min_length=MIN_LENGTH,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "    pre.process_dataset(\n",
    "        max_files=MAX_FILES,\n",
    "        save_checkpoint_every=SAVE_CHECKPOINT_EVERY\n",
    "    )\n",
    "\n",
    "    print(\"\\nRun finalize_dataset(...) when you finish all runs.\\n\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGs51bsniAai"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL SPLIT + VOCAB CREATION\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Optional\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def finalize_dataset(output_dir: str, train_ratio: float, val_ratio: float, tokenization: str):\n",
    "    output_dir = Path(output_dir)\n",
    "    raw_file = output_dir / \"raw_abc.txt\"\n",
    "\n",
    "    print(\"Loading full raw dataset...\")\n",
    "    with open(raw_file, 'r', encoding='utf-8') as f:\n",
    "        # samples = f.read().split(\"<|endoftext|>\")\n",
    "        samples = f.read()\n",
    "\n",
    "    # samples = [s.strip() for s in samples if len(s.strip()) > 0]\n",
    "    samples = [s.strip() for s in samples]\n",
    "\n",
    "    print(f\"Total samples collected: {len(samples):,}\")\n",
    "\n",
    "    # Shuffle\n",
    "    np.random.shuffle(samples)\n",
    "\n",
    "    n = len(samples)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "\n",
    "    train = samples[:n_train]\n",
    "    val = samples[n_train:n_train+n_val]\n",
    "    test = samples[n_train+n_val:]\n",
    "\n",
    "    def save_split(name, data):\n",
    "        with open(output_dir / f\"{name}.txt\", 'w', encoding='utf-8') as f:\n",
    "            for s in data:\n",
    "                # f.write(s + \"\\n<|endoftext|>\\n\")\n",
    "                f.write(s)\n",
    "        print(f\"{name}: {len(data):,} samples\")\n",
    "\n",
    "    save_split(\"train\", train)\n",
    "    save_split(\"val\", val)\n",
    "    save_split(\"test\", test)\n",
    "\n",
    "    # Build vocab\n",
    "    counter = Counter()\n",
    "    for s in train:\n",
    "        counter.update(s)\n",
    "\n",
    "    vocab = ['<PAD>', '<UNK>', '<BOS>', '<EOS>'] + list(counter.keys())\n",
    "\n",
    "    with open(output_dir / \"vocab.json\", 'w') as f:\n",
    "        json.dump(vocab, f, indent=2)\n",
    "\n",
    "    print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "    # Calculate comprehensive statistics\n",
    "    print(\"\\nCalculating statistics...\")\n",
    "\n",
    "    # Sequence lengths for all samples\n",
    "    all_lengths = [len(s) for s in samples]\n",
    "    train_lengths = [len(s) for s in train]\n",
    "    val_lengths = [len(s) for s in val]\n",
    "    test_lengths = [len(s) for s in test]\n",
    "\n",
    "    # Token counts\n",
    "    train_tokens = sum(train_lengths)\n",
    "    val_tokens = sum(val_lengths)\n",
    "    test_tokens = sum(test_lengths)\n",
    "    total_tokens = sum(all_lengths)\n",
    "\n",
    "    # Build stats dictionary\n",
    "    stats = {\n",
    "        \"dataset_info\": {\n",
    "            \"total_samples\": len(samples),\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"tokenization\": tokenization,\n",
    "            \"vocab_size\": len(vocab),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"splits\": {\n",
    "            \"train\": {\n",
    "                \"num_samples\": len(train),\n",
    "                \"num_tokens\": train_tokens,\n",
    "                \"proportion\": train_ratio\n",
    "            },\n",
    "            \"val\": {\n",
    "                \"num_samples\": len(val),\n",
    "                \"num_tokens\": val_tokens,\n",
    "                \"proportion\": val_ratio\n",
    "            },\n",
    "            \"test\": {\n",
    "                \"num_samples\": len(test),\n",
    "                \"num_tokens\": test_tokens,\n",
    "                \"proportion\": 1.0 - train_ratio - val_ratio\n",
    "            }\n",
    "        },\n",
    "        \"sequence_length_statistics\": {\n",
    "            \"overall\": {\n",
    "                \"min\": int(np.min(all_lengths)),\n",
    "                \"max\": int(np.max(all_lengths)),\n",
    "                \"mean\": float(np.mean(all_lengths)),\n",
    "                \"median\": float(np.median(all_lengths)),\n",
    "                \"std\": float(np.std(all_lengths)),\n",
    "                \"percentiles\": {\n",
    "                    \"25th\": float(np.percentile(all_lengths, 25)),\n",
    "                    \"50th\": float(np.percentile(all_lengths, 50)),\n",
    "                    \"75th\": float(np.percentile(all_lengths, 75)),\n",
    "                    \"90th\": float(np.percentile(all_lengths, 90)),\n",
    "                    \"95th\": float(np.percentile(all_lengths, 95)),\n",
    "                    \"99th\": float(np.percentile(all_lengths, 99))\n",
    "                }\n",
    "            },\n",
    "            \"train\": {\n",
    "                \"min\": int(np.min(train_lengths)),\n",
    "                \"max\": int(np.max(train_lengths)),\n",
    "                \"mean\": float(np.mean(train_lengths)),\n",
    "                \"median\": float(np.median(train_lengths))\n",
    "            },\n",
    "            \"val\": {\n",
    "                \"min\": int(np.min(val_lengths)),\n",
    "                \"max\": int(np.max(val_lengths)),\n",
    "                \"mean\": float(np.mean(val_lengths)),\n",
    "                \"median\": float(np.median(val_lengths))\n",
    "            },\n",
    "            \"test\": {\n",
    "                \"min\": int(np.min(test_lengths)),\n",
    "                \"max\": int(np.max(test_lengths)),\n",
    "                \"mean\": float(np.mean(test_lengths)),\n",
    "                \"median\": float(np.median(test_lengths))\n",
    "            }\n",
    "        },\n",
    "        \"vocabulary_statistics\": {\n",
    "            \"total_unique_chars\": len(vocab) - 4,  # Excluding special tokens\n",
    "            \"special_tokens\": ['<PAD>', '<UNK>', '<BOS>', '<EOS>'],\n",
    "            \"most_common_chars\": [\n",
    "                {\"char\": char, \"count\": int(count)}\n",
    "                for char, count in counter.most_common(20)\n",
    "            ]\n",
    "        },\n",
    "        \"data_quality\": {\n",
    "            \"empty_samples_filtered\": 0,  # Adjust if you track this\n",
    "            \"samples_with_special_chars\": sum(\n",
    "                1 for s in samples if any(c in s for c in ['<', '>', '|'])\n",
    "            ),\n",
    "            \"average_unique_chars_per_sample\": float(\n",
    "                np.mean([len(set(s)) for s in samples])\n",
    "            )\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Save stats\n",
    "    stats_file = output_dir / \"stats.json\"\n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"STATISTICS SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total samples:        {stats['dataset_info']['total_samples']:,}\")\n",
    "    print(f\"Total tokens:         {stats['dataset_info']['total_tokens']:,}\")\n",
    "    print(f\"Vocabulary size:      {stats['dataset_info']['vocab_size']:,}\")\n",
    "    print(f\"\\nTrain samples:        {stats['splits']['train']['num_samples']:,}\")\n",
    "    print(f\"Train tokens:         {stats['splits']['train']['num_tokens']:,}\")\n",
    "    print(f\"Val samples:          {stats['splits']['val']['num_samples']:,}\")\n",
    "    print(f\"Val tokens:           {stats['splits']['val']['num_tokens']:,}\")\n",
    "    print(f\"Test samples:         {stats['splits']['test']['num_samples']:,}\")\n",
    "    print(f\"Test tokens:          {stats['splits']['test']['num_tokens']:,}\")\n",
    "    print(f\"\\nMean sequence length: {stats['sequence_length_statistics']['overall']['mean']:.0f}\")\n",
    "    print(f\"Median sequence length: {stats['sequence_length_statistics']['overall']['median']:.0f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Saved statistics to {stats_file}\")\n",
    "    print(\"\\nFINAL DATASET READY.\")\n",
    "\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/processed_music_data\"\n",
    "\n",
    "finalize_dataset(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    train_ratio=0.98,\n",
    "    val_ratio=0.01,\n",
    "    tokenization=\"character\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO8IN8b95EbCH8f6cqLaPPY",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
