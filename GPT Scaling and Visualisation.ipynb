{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYle4sezh3dK"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transformer Scaling Study - Complete Training Pipeline\n",
    "Based on nanoGPT architecture with modifications for music data\n",
    "\n",
    "CS-GY 6923 Scaling Laws Project\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL ARCHITECTURE (Based on nanoGPT)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"GPT model configuration\"\"\"\n",
    "    vocab_size: int = 256\n",
    "    block_size: int = 256  # context length\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = False\n",
    "\n",
    "    def get_param_count(self):\n",
    "        \"\"\"Estimate parameter count\"\"\"\n",
    "        # Embedding\n",
    "        params = self.vocab_size * self.n_embd\n",
    "        # Transformer blocks\n",
    "        params += self.n_layer * (\n",
    "            4 * self.n_embd * self.n_embd +  # attention\n",
    "            8 * self.n_embd * self.n_embd     # FFN (4x expansion)\n",
    "        )\n",
    "        # Output projection\n",
    "        params += self.vocab_size * self.n_embd\n",
    "        return params\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head causal self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # Key, query, value projections\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # Regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        # Causal mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                            .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch, sequence length, embedding dim\n",
    "\n",
    "        # Calculate query, key, values\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # Output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Feed-forward network\"\"\"\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block\"\"\"\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"GPT Language Model\"\"\"\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # Weight tying\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # Report number of parameters\n",
    "        print(f\"Model parameters: {self.get_num_params():,}\")\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Sequence length {t} exceeds block size {self.config.block_size}\"\n",
    "\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # Calculate loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def get_num_params(self):\n",
    "        \"\"\"Count total parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Generate new tokens\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop to block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # Forward pass\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # Top-k sampling\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # Sample\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL CONFIGURATIONS FOR SCALING STUDY\n",
    "# =============================================================================\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    'tiny': GPTConfig(\n",
    "        vocab_size=256,\n",
    "        block_size=256,\n",
    "        n_layer=2,\n",
    "        n_head=2,\n",
    "        n_embd=64,\n",
    "        dropout=0.1\n",
    "    ),\n",
    "    'small': GPTConfig(\n",
    "        vocab_size=256,\n",
    "        block_size=256, # context\n",
    "        n_layer=4, # nl\n",
    "        n_head=4, # nh\n",
    "        n_embd=128, # dmodel\n",
    "        dropout=0.1\n",
    "    ),\n",
    "    'medium': GPTConfig(\n",
    "        vocab_size=256,\n",
    "        block_size=256,\n",
    "        n_layer=6,\n",
    "        n_head=6,\n",
    "        n_embd=252,\n",
    "        dropout=0.1\n",
    "    ),\n",
    "    'large': GPTConfig(\n",
    "        vocab_size=256,\n",
    "        block_size=256,\n",
    "        n_layer=8,\n",
    "        n_head=8,\n",
    "        n_embd=384,\n",
    "        dropout=0.1\n",
    "    ),\n",
    "    'xl': GPTConfig(\n",
    "        vocab_size=256,\n",
    "        block_size=256,\n",
    "        n_layer=12,\n",
    "        n_head=12,\n",
    "        n_embd=516,\n",
    "        dropout=0.1\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET\n",
    "# =============================================================================\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    \"\"\"Music text dataset\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, vocab_path: str, block_size: int):\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "        # Load data\n",
    "        print(f\"Loading data from {data_path}...\")\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Tokenize\n",
    "        print(\"Tokenizing...\")\n",
    "        self.tokens = self.encode(text)\n",
    "        print(f\"Dataset has {len(self.tokens):,} tokens\")\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Encode text to token indices\"\"\"\n",
    "        return [self.char_to_idx.get(ch, self.char_to_idx['<UNK>']) for ch in text]\n",
    "\n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        \"\"\"Decode token indices to text\"\"\"\n",
    "        return ''.join([self.idx_to_char[i] for i in indices])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get chunk of data\n",
    "        chunk = self.tokens[idx:idx + self.block_size + 1]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration\"\"\"\n",
    "    # Data\n",
    "    data_dir: str = \"/content/drive/MyDrive/processed_music_data\"\n",
    "    vocab_file: str = \"vocab.json\"\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 0.1\n",
    "    beta1: float = 0.9\n",
    "    beta2: float = 0.95\n",
    "    grad_clip: float = 1.0\n",
    "    warmup_steps: int = 2000\n",
    "    max_epochs: int = 1\n",
    "\n",
    "    # Evaluation\n",
    "    eval_interval: int = 500\n",
    "    eval_iters: int = 100\n",
    "\n",
    "    # System\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    compile_model: bool = False  # torch.compile for speed\n",
    "\n",
    "    # Logging\n",
    "    output_dir: str = \"/content/drive/MyDrive/models/checkpoints\"\n",
    "\n",
    "\n",
    "def get_lr(step: int, config: TrainingConfig, total_steps: int):\n",
    "    \"\"\"Learning rate schedule with warmup and cosine decay\"\"\"\n",
    "    # Warmup\n",
    "    if step < config.warmup_steps:\n",
    "        return config.learning_rate * step / config.warmup_steps\n",
    "    # Cosine decay\n",
    "    progress = (step - config.warmup_steps) / (total_steps - config.warmup_steps)\n",
    "    return config.learning_rate * 0.1 + 0.9 * config.learning_rate * (1 + math.cos(math.pi * progress)) / 2\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_loader, eval_iters, device):\n",
    "    \"\"\"Estimate loss on evaluation set\"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    for i, (x, y) in enumerate(eval_loader):\n",
    "        if i >= eval_iters:\n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    model.train()\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: GPT,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    config: TrainingConfig,\n",
    "    epoch: int,\n",
    "    total_steps: int\n",
    "):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    train_history = []\n",
    "    step = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(pbar):\n",
    "        x, y = x.to(config.device), y.to(config.device)\n",
    "\n",
    "        # Forward pass\n",
    "        _, loss = model(x, y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update learning rate\n",
    "        lr = get_lr(step, config, total_steps)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # Track metrics\n",
    "        losses.append(loss.item())\n",
    "        train_history.append({\n",
    "            'step': step,\n",
    "            'loss': loss.item(),\n",
    "            'lr': lr\n",
    "        })\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'lr': f'{lr:.2e}'\n",
    "        })\n",
    "\n",
    "        # Periodic evaluation\n",
    "        if step % config.eval_interval == 0 and step > 0:\n",
    "            val_loss = estimate_loss(model, val_loader, config.eval_iters, config.device)\n",
    "            print(f\"\\nStep {step}: val_loss = {val_loss:.4f}\")\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return np.mean(losses), train_history\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model_name: str,\n",
    "    config: GPTConfig,\n",
    "    train_config: TrainingConfig\n",
    "):\n",
    "    \"\"\"Complete training pipeline for one model\"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {model_name.upper()} model\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Update vocab size from data\n",
    "    vocab_path = Path(train_config.data_dir) / train_config.vocab_file\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    config.vocab_size = len(vocab)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = MusicDataset(\n",
    "        Path(train_config.data_dir) / \"train.txt\",\n",
    "        vocab_path,\n",
    "        config.block_size\n",
    "    )\n",
    "\n",
    "    val_dataset = MusicDataset(\n",
    "        Path(train_config.data_dir) / \"val.txt\",\n",
    "        vocab_path,\n",
    "        config.block_size\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=train_config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=train_config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Create model\n",
    "    model = GPT(config).to(train_config.device)\n",
    "\n",
    "    # Count parameters\n",
    "    n_params = model.get_num_params()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=train_config.learning_rate,\n",
    "        betas=(train_config.beta1, train_config.beta2),\n",
    "        weight_decay=train_config.weight_decay\n",
    "    )\n",
    "\n",
    "    # Calculate total steps\n",
    "    total_steps = len(train_loader) * train_config.max_epochs\n",
    "\n",
    "    # Training tracking\n",
    "    start_time = time.time()\n",
    "    all_train_history = []\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(train_config.max_epochs):\n",
    "        train_loss, train_history = train_epoch(\n",
    "            model, train_loader, val_loader, optimizer,\n",
    "            train_config, epoch + 1, total_steps\n",
    "        )\n",
    "        all_train_history.extend(train_history)\n",
    "\n",
    "    # Final evaluation\n",
    "    print(\"\\nFinal evaluation...\")\n",
    "    val_loss = estimate_loss(model, val_loader, len(val_loader), train_config.device)\n",
    "\n",
    "    # Calculate metrics\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Get GPU memory if available\n",
    "    gpu_memory = 0\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.max_memory_allocated() / 1024**3  # GB\n",
    "\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'n_params': n_params,\n",
    "        'config': asdict(config),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'training_time': training_time,\n",
    "        'gpu_memory_gb': gpu_memory,\n",
    "        'train_history': all_train_history\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    output_dir = Path(train_config.output_dir) / model_name\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), output_dir / \"model.pt\")\n",
    "\n",
    "    # Save config\n",
    "    with open(output_dir / \"config.json\", 'w') as f:\n",
    "        json.dump(asdict(config), f, indent=2)\n",
    "\n",
    "    # Save results\n",
    "    with open(output_dir / \"results.json\", 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRAINING COMPLETE: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Parameters:     {n_params:,}\")\n",
    "    print(f\"Train loss:     {train_loss:.4f}\")\n",
    "    print(f\"Val loss:       {val_loss:.4f}\")\n",
    "    print(f\"Training time:  {training_time/60:.1f} minutes\")\n",
    "    print(f\"GPU memory:     {gpu_memory:.2f} GB\")\n",
    "    print(f\"Saved to:       {output_dir}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING SCRIPT\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Train all models for scaling study\"\"\"\n",
    "\n",
    "    # Configuration\n",
    "    train_config = TrainingConfig(\n",
    "        data_dir=\"/content/drive/MyDrive/processed_music_data\",\n",
    "        batch_size=32,\n",
    "        learning_rate=3e-4,\n",
    "        max_epochs=1,\n",
    "        output_dir=\"/content/drive/MyDrive/models/checkpoints\"\n",
    "    )\n",
    "\n",
    "    print(f\"Device: {train_config.device}\")\n",
    "    print(f\"Data directory: {train_config.data_dir}\")\n",
    "    print(f\"Output directory: {train_config.output_dir}\")\n",
    "\n",
    "    # Train all models\n",
    "    all_results = {}\n",
    "\n",
    "    for model_name, config in MODEL_CONFIGS.items():\n",
    "        results = train_model(model_name, config, train_config)\n",
    "        all_results[model_name] = results\n",
    "\n",
    "        # Clear GPU memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Save combined results\n",
    "    output_dir = Path(train_config.output_dir)\n",
    "    output_file = output_dir / \"all_results.json\"\n",
    "\n",
    "    # Step 1 — read existing dict if present\n",
    "    if output_file.exists():\n",
    "        with open(output_file, \"r\") as f:\n",
    "            old_data = json.load(f)\n",
    "    else:\n",
    "        old_data = {}\n",
    "\n",
    "    # Step 2 — ensure new data is dict\n",
    "    if not isinstance(all_results, dict):\n",
    "        raise ValueError(\"all_results must be a dictionary when prepending to an object.\")\n",
    "\n",
    "    # Step 3 — prepend: new keys first, then old keys\n",
    "    combined = {**all_results, **old_data}\n",
    "\n",
    "    # Step 4 — save back\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(combined, f, indent=2)\n",
    "\n",
    "    # with open(output_dir / \"all_results.json\", 'w') as f:\n",
    "    #     json.dump(all_results, f, indent=2)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ALL MODELS TRAINED - SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Model':<10} {'Params':<12} {'Val Loss':<10} {'Time (min)':<12}\")\n",
    "    print(\"-\"*70)\n",
    "    for name, res in all_results.items():\n",
    "        print(f\"{name:<10} {res['n_params']:>11,} {res['val_loss']:>10.4f} {res['training_time']/60:>11.1f}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"\\nResults saved to: {output_dir / 'all_results.json'}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlC1PIiqyuTG"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scaling Law Analysis for Transformer Models\n",
    "Analyzes training results and generates scaling plots\n",
    "\n",
    "CS-GY 6923 Scaling Laws Project\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.optimize import curve_fit\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# =============================================================================\n",
    "# SCALING LAW FITTING\n",
    "# =============================================================================\n",
    "\n",
    "def power_law(N, a, alpha, c):\n",
    "    \"\"\"\n",
    "    Power law function: L = a * N^(-alpha) + c\n",
    "\n",
    "    Args:\n",
    "        N: Model parameter count\n",
    "        a: Scaling coefficient\n",
    "        alpha: Scaling exponent\n",
    "        c: Irreducible loss offset\n",
    "    \"\"\"\n",
    "    return a * np.power(N, -alpha) + c\n",
    "\n",
    "\n",
    "def fit_scaling_law(param_counts: np.ndarray, losses: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Fit power law to scaling data\n",
    "\n",
    "    Returns:\n",
    "        fitted_params: [a, alpha, c]\n",
    "        fit_info: Dictionary with fit statistics\n",
    "    \"\"\"\n",
    "    # Initial guess for parameters\n",
    "    p0 = [1.0, 0.1, 0.5]\n",
    "\n",
    "    # Fit curve\n",
    "    try:\n",
    "        popt, pcov = curve_fit(\n",
    "            power_law,\n",
    "            param_counts,\n",
    "            losses,\n",
    "            p0=p0,\n",
    "            maxfev=10000\n",
    "        )\n",
    "\n",
    "        # Calculate R-squared\n",
    "        residuals = losses - power_law(param_counts, *popt)\n",
    "        ss_res = np.sum(residuals**2)\n",
    "        ss_tot = np.sum((losses - np.mean(losses))**2)\n",
    "        r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "        # Standard errors\n",
    "        perr = np.sqrt(np.diag(pcov))\n",
    "\n",
    "        fit_info = {\n",
    "            'a': popt[0],\n",
    "            'alpha': popt[1],\n",
    "            'c': popt[2],\n",
    "            'a_stderr': perr[0],\n",
    "            'alpha_stderr': perr[1],\n",
    "            'c_stderr': perr[2],\n",
    "            'r_squared': r_squared\n",
    "        }\n",
    "\n",
    "        return popt, fit_info\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting scaling law: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PLOTTING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def plot_scaling_law(results_dict: Dict, output_dir: Path):\n",
    "    \"\"\"\n",
    "    Create main scaling law plot with power law fit\n",
    "    \"\"\"\n",
    "    # Extract data\n",
    "    models = []\n",
    "    param_counts = []\n",
    "    val_losses = []\n",
    "\n",
    "    for model_name, results in results_dict.items():\n",
    "        models.append(model_name)\n",
    "        param_counts.append(results['n_params'])\n",
    "        val_losses.append(results['val_loss'])\n",
    "\n",
    "    param_counts = np.array(param_counts)\n",
    "    val_losses = np.array(val_losses)\n",
    "\n",
    "    # Sort by parameter count\n",
    "    sort_idx = np.argsort(param_counts)\n",
    "    param_counts = param_counts[sort_idx]\n",
    "    val_losses = val_losses[sort_idx]\n",
    "    models = [models[i] for i in sort_idx]\n",
    "\n",
    "    # Fit scaling law\n",
    "    popt, fit_info = fit_scaling_law(param_counts, val_losses)\n",
    "\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot data points\n",
    "    ax.scatter(param_counts, val_losses, s=100, alpha=0.7,\n",
    "               c='steelblue', edgecolors='black', linewidth=1.5,\n",
    "               label='Observed', zorder=3)\n",
    "\n",
    "    # Add model labels\n",
    "    for i, (n, l, name) in enumerate(zip(param_counts, val_losses, models)):\n",
    "        ax.annotate(name, (n, l),\n",
    "                   xytext=(10, 5), textcoords='offset points',\n",
    "                   fontsize=9, alpha=0.8)\n",
    "\n",
    "    if popt is not None:\n",
    "        # Generate smooth curve for fitted line\n",
    "        N_smooth = np.logspace(np.log10(param_counts.min()),\n",
    "                               np.log10(param_counts.max()),\n",
    "                               100)\n",
    "        L_fitted = power_law(N_smooth, *popt)\n",
    "\n",
    "        ax.plot(N_smooth, L_fitted, 'r--', linewidth=2,\n",
    "                label=f'Power Law Fit', alpha=0.8, zorder=2)\n",
    "\n",
    "        # Add fit equation to plot\n",
    "        eq_text = f'$L = {popt[0]:.3f} \\\\cdot N^{{-{popt[1]:.3f}}} + {popt[2]:.3f}$\\n'\n",
    "        eq_text += f'$\\\\alpha = {fit_info[\"alpha\"]:.4f} \\\\pm {fit_info[\"alpha_stderr\"]:.4f}$\\n'\n",
    "        eq_text += f'$R^2 = {fit_info[\"r_squared\"]:.4f}$'\n",
    "\n",
    "        ax.text(0.05, 0.95, eq_text, transform=ax.transAxes,\n",
    "                fontsize=11, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Number of Parameters (N)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Validation Loss (L)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Neural Scaling Law: Model Size vs. Performance',\n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.legend(fontsize=10, loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'scaling_law.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved scaling law plot to {output_dir / 'scaling_law.png'}\")\n",
    "\n",
    "    # Print fit results\n",
    "    if fit_info is not None:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SCALING LAW FIT RESULTS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Power Law: L = a * N^(-α) + c\")\n",
    "        print(f\"\\nFitted Parameters:\")\n",
    "        print(f\"  a (coefficient):      {fit_info['a']:.6f} ± {fit_info['a_stderr']:.6f}\")\n",
    "        print(f\"  α (exponent):         {fit_info['alpha']:.6f} ± {fit_info['alpha_stderr']:.6f}\")\n",
    "        print(f\"  c (irreducible loss): {fit_info['c']:.6f} ± {fit_info['c_stderr']:.6f}\")\n",
    "        print(f\"\\nGoodness of Fit:\")\n",
    "        print(f\"  R² = {fit_info['r_squared']:.6f}\")\n",
    "        print(f\"\\nInterpretation:\")\n",
    "        print(f\"  • Scaling exponent α = {fit_info['alpha']:.4f} indicates that\")\n",
    "        print(f\"    validation loss decreases as N^(-{fit_info['alpha']:.4f})\")\n",
    "        print(f\"  • To halve the loss, you need ~{2**(1/fit_info['alpha']):.1f}× more parameters\")\n",
    "        print(f\"  • Irreducible loss c = {fit_info['c']:.4f} represents the\")\n",
    "        print(f\"    theoretical minimum achievable loss\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "        # Save fit results\n",
    "        with open(output_dir / 'scaling_law_fit.json', 'w') as f:\n",
    "            json.dump(fit_info, f, indent=2)\n",
    "\n",
    "    return fig, fit_info\n",
    "\n",
    "\n",
    "def plot_training_curves(results_dict: Dict, output_dir: Path):\n",
    "    \"\"\"\n",
    "    Plot training loss curves over time for all models\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(results_dict)))\n",
    "\n",
    "    for (model_name, results), color in zip(results_dict.items(), colors):\n",
    "        if 'train_history' in results and results['train_history']:\n",
    "            history = results['train_history']\n",
    "            steps = [h['step'] for h in history]\n",
    "            losses = [h['loss'] for h in history]\n",
    "\n",
    "            # Plot with smoothing (moving average)\n",
    "            window = max(1, len(losses) // 50)\n",
    "            if window > 1:\n",
    "                losses_smooth = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "                steps_smooth = steps[:len(losses_smooth)]\n",
    "                ax.plot(steps_smooth, losses_smooth, label=f\"{model_name} ({results['n_params']:,} params)\",\n",
    "                       linewidth=2, color=color, alpha=0.8)\n",
    "            else:\n",
    "                ax.plot(steps, losses, label=f\"{model_name} ({results['n_params']:,} params)\",\n",
    "                       linewidth=2, color=color, alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel('Training Step', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Training Loss', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Training Loss Curves by Model Size', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.legend(fontsize=9, loc='best')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved training curves to {output_dir / 'training_curves.png'}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_training_metrics(results_dict: Dict, output_dir: Path):\n",
    "    \"\"\"\n",
    "    Plot training time and GPU memory usage\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Extract data\n",
    "    models = []\n",
    "    param_counts = []\n",
    "    training_times = []\n",
    "    gpu_memory = []\n",
    "\n",
    "    for model_name, results in results_dict.items():\n",
    "        models.append(model_name)\n",
    "        param_counts.append(results['n_params'])\n",
    "        training_times.append(results['training_time'] / 60)  # Convert to minutes\n",
    "        gpu_memory.append(results.get('gpu_memory_gb', 0))\n",
    "\n",
    "    # Sort by parameter count\n",
    "    param_counts = np.array(param_counts)\n",
    "    sort_idx = np.argsort(param_counts)\n",
    "    param_counts = param_counts[sort_idx]\n",
    "    training_times = [training_times[i] for i in sort_idx]\n",
    "    gpu_memory = [gpu_memory[i] for i in sort_idx]\n",
    "    models = [models[i] for i in sort_idx]\n",
    "\n",
    "    # Plot 1: Training Time\n",
    "    ax1.scatter(param_counts, training_times, s=100, alpha=0.7,\n",
    "               c='coral', edgecolors='black', linewidth=1.5)\n",
    "    for i, (n, t, name) in enumerate(zip(param_counts, training_times, models)):\n",
    "        ax1.annotate(name, (n, t),\n",
    "                    xytext=(10, 5), textcoords='offset points',\n",
    "                    fontsize=9, alpha=0.8)\n",
    "\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_xlabel('Number of Parameters (N)', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('Training Time (minutes)', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('Wall-Clock Training Time vs. Model Size', fontsize=12, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "    # Plot 2: GPU Memory\n",
    "    if any(m > 0 for m in gpu_memory):\n",
    "        ax2.scatter(param_counts, gpu_memory, s=100, alpha=0.7,\n",
    "                   c='mediumseagreen', edgecolors='black', linewidth=1.5)\n",
    "        for i, (n, m, name) in enumerate(zip(param_counts, gpu_memory, models)):\n",
    "            ax2.annotate(name, (n, m),\n",
    "                        xytext=(10, 5), textcoords='offset points',\n",
    "                        fontsize=9, alpha=0.8)\n",
    "\n",
    "        ax2.set_xscale('log')\n",
    "        ax2.set_xlabel('Number of Parameters (N)', fontsize=11, fontweight='bold')\n",
    "        ax2.set_ylabel('Peak GPU Memory (GB)', fontsize=11, fontweight='bold')\n",
    "        ax2.set_title('GPU Memory Usage vs. Model Size', fontsize=12, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No GPU memory data available',\n",
    "                transform=ax2.transAxes, ha='center', va='center',\n",
    "                fontsize=12, style='italic')\n",
    "        ax2.set_xticks([])\n",
    "        ax2.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved training metrics to {output_dir / 'training_metrics.png'}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_loss_comparison(results_dict: Dict, output_dir: Path):\n",
    "    \"\"\"\n",
    "    Compare training vs validation loss across models\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    models = []\n",
    "    param_counts = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for model_name, results in results_dict.items():\n",
    "        models.append(model_name)\n",
    "        param_counts.append(results['n_params'])\n",
    "        train_losses.append(results['train_loss'])\n",
    "        val_losses.append(results['val_loss'])\n",
    "\n",
    "    # Sort by parameter count\n",
    "    param_counts = np.array(param_counts)\n",
    "    sort_idx = np.argsort(param_counts)\n",
    "    param_counts = param_counts[sort_idx]\n",
    "    train_losses = [train_losses[i] for i in sort_idx]\n",
    "    val_losses = [val_losses[i] for i in sort_idx]\n",
    "    models = [models[i] for i in sort_idx]\n",
    "\n",
    "    # Plot\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax.bar(x - width/2, train_losses, width, label='Training Loss',\n",
    "                   color='skyblue', edgecolor='black', linewidth=1.2)\n",
    "    bars2 = ax.bar(x + width/2, val_losses, width, label='Validation Loss',\n",
    "                   color='lightcoral', edgecolor='black', linewidth=1.2)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Training vs. Validation Loss by Model', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f\"{m}\\n({p:,})\" for m, p in zip(models, param_counts)], fontsize=9)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'loss_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved loss comparison to {output_dir / 'loss_comparison.png'}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_summary_table(results_dict: Dict, output_dir: Path):\n",
    "    \"\"\"\n",
    "    Create a summary table of all metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"MODEL TRAINING SUMMARY\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"{'Model':<10} {'Parameters':<15} {'Train Loss':<12} {'Val Loss':<12} {'Time (min)':<12} {'GPU (GB)':<10}\")\n",
    "    print(\"-\"*90)\n",
    "\n",
    "    # Sort by parameter count\n",
    "    sorted_items = sorted(results_dict.items(), key=lambda x: x[1]['n_params'])\n",
    "\n",
    "    for model_name, results in sorted_items:\n",
    "        print(f\"{model_name:<10} {results['n_params']:>14,} \"\n",
    "              f\"{results['train_loss']:>11.4f} \"\n",
    "              f\"{results['val_loss']:>11.4f} \"\n",
    "              f\"{results['training_time']/60:>11.1f} \"\n",
    "              f\"{results.get('gpu_memory_gb', 0):>9.2f}\")\n",
    "\n",
    "    print(\"=\"*90 + \"\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN ANALYSIS SCRIPT\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Run complete scaling analysis\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    results_file = Path(\"/content/drive/MyDrive/models/checkpoints/all_results.json\")\n",
    "    output_dir = Path(\"/content/drive/MyDrive/models/analysis\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Loading results from: {results_file}\")\n",
    "\n",
    "    # Load results\n",
    "    with open(results_file, 'r') as f:\n",
    "        results_dict = json.load(f)\n",
    "\n",
    "    print(f\"Found results for {len(results_dict)} models\\n\")\n",
    "\n",
    "    # Create summary table\n",
    "    create_summary_table(results_dict, output_dir)\n",
    "\n",
    "    # Generate all plots\n",
    "    print(\"Generating plots...\\n\")\n",
    "\n",
    "    # 1. Main scaling law plot\n",
    "    plot_scaling_law(results_dict, output_dir)\n",
    "\n",
    "    # 2. Training curves\n",
    "    plot_training_curves(results_dict, output_dir)\n",
    "\n",
    "    # 3. Training metrics (time and memory)\n",
    "    plot_training_metrics(results_dict, output_dir)\n",
    "\n",
    "    # 4. Loss comparison\n",
    "    plot_loss_comparison(results_dict, output_dir)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"All plots saved to: {output_dir}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMcbDR37UxOB+FoSbdnZ3J3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
